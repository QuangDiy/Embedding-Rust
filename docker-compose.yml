services:
  model-downloader-rs:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: model-downloader-rs
    command: /app/download_models
    volumes:
      - ./model_repository:/app/model_repository
    networks:
      - embedding-network
    restart: "no"

  embedding-rust-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: embedding-rust-api
    ports:
      - "8000:8000"
    environment:
      - TRITON_URL=triton:8000
      - TRITON_HTTP_CONNECTION_TIMEOUT=300
      - TRITON_HTTP_NETWORK_TIMEOUT=300
      - EMBEDDING_MODEL_NAME=jina-embeddings-v3
      - RERANKER_MODEL_NAME=jina-reranker-v2
      - TOKENIZER_PATH=jinaai/jina-embeddings-v3
      - RERANKER_TOKENIZER_PATH=jinaai/jina-reranker-v2-base-multilingual
      - TOKENIZER_FILE=/app/model_repository/jina-embeddings-v3/1/tokenizer.json
      - RERANKER_TOKENIZER_FILE=/app/model_repository/jina-reranker-v2/1/tokenizer.json
      - MAX_SEQUENCE_LENGTH=8192
      - RERANKER_MAX_SEQUENCE_LENGTH=1024
      - EMBEDDING_CLIENT_MAX_BATCH=8
      - RUST_LOG=info
      - API_KEY=1234
      - REQUIRE_API_KEY=true
    volumes:
      - ./model_repository:/app/model_repository
    depends_on:
      triton:
        condition: service_healthy
    networks:
      - embedding-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  triton:
    image: nvcr.io/nvidia/tritonserver:24.08-py3
    container_name: triton-server-rs
    command: tritonserver --model-repository=/models --strict-model-config=false
    depends_on:
      model-downloader-rs:
        condition: service_completed_successfully
    ports:
      - "8001:8000"  # HTTP (Triton uses 8000 internally)
      - "8002:8001"  # gRPC
      - "8003:8002"  # Metrics
    volumes:
      - ./model_repository:/models
    networks:
      - embedding-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    # GPU deployment (GPU-0 enabled)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']  # Specifically use GPU-0
              capabilities: [gpu]

networks:
  embedding-network:
    driver: bridge
